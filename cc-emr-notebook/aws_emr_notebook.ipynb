{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1G', 'executorMemory': '1G', 'conf': {'spark.python.profile': 'true'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"1G\", \"executorMemory\" : \"1G\", \"conf\": {\"spark.python.profile\": \"true\"} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1G', 'executorMemory': '1G', 'conf': {'spark.python.profile': 'true'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b68d5cf4814f6395cb4a27bbdadb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list = [\"s3://commoncrawl/crawl-001/2008/07/22/3/1216753395382_3.arc.gz\",\n",
    "\"s3://commoncrawl/crawl-002/2009/09/17/12/1253234344648_12.arc.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65134f078c434bb3847306d889abb5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Args:\n",
    "    warc_parse_http_header = True\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    num_input_partitions = 10\n",
    "    num_output_partitions = 2\n",
    "    output = \"s3://emr-arc-notebook/test_arc_output\"\n",
    "    output_format = \"parquet\"\n",
    "    output_compression = \"gzip\"\n",
    "    output_option = []\n",
    "    local_temp_dir = None\n",
    "    spark_profiler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2156299fbe84a1eb31c3fe928fa8431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "from io import BytesIO\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.recordloader import ArchiveLoadFailed\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "import tempfile\n",
    "\n",
    "LOGGING_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n",
    "\n",
    "    \n",
    "class JupyterCCSparkJob(object):\n",
    "    \"\"\"\n",
    "    A simple Spark job definition to process Common Crawl data\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'CCSparkJob'\n",
    "\n",
    "    output_schema = StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"val\", LongType(), True)\n",
    "    ])\n",
    "\n",
    "    # description of input and output shown in --help\n",
    "    input_descr = \"Path to file listing input paths\"\n",
    "    output_descr = \"Name of output table (saved in spark.sql.warehouse.dir)\"\n",
    "\n",
    "    warc_parse_http_header = True\n",
    "\n",
    "    args = None\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    log_level = 'INFO'\n",
    "    logging.basicConfig(level=log_level, format=LOGGING_FORMAT)\n",
    "\n",
    "\n",
    "    def parse_arguments(self):\n",
    "        \"\"\" Returns the parsed arguments from the command line \"\"\"\n",
    "\n",
    "        description = self.name\n",
    "        if self.__doc__ is not None:\n",
    "            description += \" - \"\n",
    "            description += self.__doc__\n",
    "        args = Args()\n",
    "        return args\n",
    "\n",
    "    def add_arguments(self, parser):\n",
    "        pass\n",
    "\n",
    "    def validate_arguments(self, args):\n",
    "        if \"orc\" == args.output_format and \"gzip\" == args.output_compression:\n",
    "            # gzip for Parquet, zlib for ORC\n",
    "            args.output_compression = \"zlib\"\n",
    "        return True\n",
    "\n",
    "    def get_output_options(self):\n",
    "        return {x[0]: x[1] for x in map(lambda x: x.split('=', 1),\n",
    "                                        self.args.output_option)}\n",
    "\n",
    "    def init_logging(self, level=None):\n",
    "        if level is None:\n",
    "            level = self.log_level\n",
    "        else:\n",
    "            self.log_level = level\n",
    "        logging.basicConfig(level=level, format=LOGGING_FORMAT)\n",
    "\n",
    "    def init_accumulators(self, sc):\n",
    "        self.records_processed = sc.accumulator(0)\n",
    "        self.warc_input_processed = sc.accumulator(0)\n",
    "        self.warc_input_failed = sc.accumulator(0)\n",
    "\n",
    "    def get_logger(self, spark_context=None):\n",
    "        \"\"\"Get logger from SparkContext or (if None) from logging module\"\"\"\n",
    "        if spark_context is None:\n",
    "            return logging.getLogger(self.name)\n",
    "        return spark_context._jvm.org.apache.log4j.LogManager \\\n",
    "            .getLogger(self.name)\n",
    "\n",
    "    def run(self):\n",
    "        self.args = self.parse_arguments()\n",
    "        \n",
    "        conf = SparkConf()\n",
    "        \n",
    "        if self.args.spark_profiler:\n",
    "            conf = conf.set(\"spark.python.profile\", \"true\")\n",
    "        \n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        \n",
    "        \n",
    "        sqlc = SQLContext(sparkContext=sc)\n",
    "\n",
    "        self.init_accumulators(sc)\n",
    "\n",
    "        self.run_job(sc, sqlc)\n",
    "\n",
    "        if self.args.spark_profiler:\n",
    "            sc.show_profiles()\n",
    "\n",
    "        sc.stop()\n",
    "\n",
    "    def log_aggregator(self, sc, agg, descr):\n",
    "        self.get_logger(sc).info(descr.format(agg.value))\n",
    "\n",
    "    def log_aggregators(self, sc):\n",
    "        self.log_aggregator(sc, self.warc_input_processed,\n",
    "                            'WARC/WAT/WET input files processed = {}')\n",
    "        self.log_aggregator(sc, self.warc_input_failed,\n",
    "                            'WARC/WAT/WET input files failed = {}')\n",
    "        self.log_aggregator(sc, self.records_processed,\n",
    "                            'WARC/WAT/WET records processed = {}')\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_by_key_func(a, b):\n",
    "        return a + b\n",
    "\n",
    "    def run_job(self, sc, sqlc):\n",
    "        \n",
    "        input_data = sc.parallelize(file_list)\n",
    "           \n",
    "        output = input_data.mapPartitionsWithIndex(self.process_warcs)\n",
    "        sqlc.createDataFrame(output, schema=self.output_schema) \\\n",
    "            .coalesce(self.args.num_output_partitions) \\\n",
    "            .write \\\n",
    "            .format(self.args.output_format) \\\n",
    "            .option(\"compression\", self.args.output_compression) \\\n",
    "            .options(**self.get_output_options()) \\\n",
    "            .parquet(self.args.output)\n",
    "    \n",
    "        self.log_aggregators(sc)\n",
    "    \n",
    "    def process_warcs(self, id_, iterator):\n",
    "        s3pattern = re.compile('^s3://([^/]+)/(.+)')\n",
    "        base_dir = \"/user/\"\n",
    "\n",
    "        # S3 client (not thread-safe, initialize outside parallelized loop)\n",
    "        no_sign_request = botocore.client.Config(\n",
    "            signature_version=botocore.UNSIGNED)\n",
    "        s3client = boto3.client('s3', config=no_sign_request)\n",
    "       \n",
    "        for uri in iterator:\n",
    "            self.warc_input_processed.add(1)\n",
    "            if uri.startswith('s3://'):\n",
    "                self.get_logger().info('Reading from S3 {}'.format(uri))\n",
    "                s3match = s3pattern.match(uri)\n",
    "                if s3match is None:\n",
    "                    self.get_logger().error(\"Invalid S3 URI: \" + uri)\n",
    "                    continue\n",
    "                bucketname = s3match.group(1)\n",
    "                path = s3match.group(2)\n",
    "                warctemp = TemporaryFile(mode='w+b',\n",
    "                                         dir=self.args.local_temp_dir)\n",
    "                try:\n",
    "                    s3client.download_fileobj(bucketname, path, warctemp)\n",
    "                except botocore.client.ClientError as exception:\n",
    "                    self.get_logger().error(\n",
    "                        'Failed to download {}: {}'.format(uri, exception))\n",
    "                    self.warc_input_failed.add(1)\n",
    "                    warctemp.close()\n",
    "                    continue\n",
    "                warctemp.seek(0)\n",
    "                stream = warctemp\n",
    "            elif uri.startswith('hdfs://'):\n",
    "                self.get_logger().error(\"HDFS input not implemented: \" + uri)\n",
    "                continue\n",
    "            else:\n",
    "                self.get_logger().info('Reading local stream {}'.format(uri))\n",
    "                if uri.startswith('file:'):\n",
    "                    uri = uri[5:]\n",
    "                uri = os.path.join(base_dir, uri)\n",
    "                try:\n",
    "                    stream = open(uri, 'rb')\n",
    "                except IOError as exception:\n",
    "                    self.get_logger().error(\n",
    "                        'Failed to open {}: {}'.format(uri, exception))\n",
    "                    self.warc_input_failed.add(1)\n",
    "                    continue\n",
    "\n",
    "            no_parse = (not self.warc_parse_http_header)\n",
    "           \n",
    "            try:\n",
    "                archive_iterator = ArchiveIterator(stream,\n",
    "                                                   no_record_parse=no_parse, arc2warc = True)\n",
    "                \n",
    "                for res in self.iterate_records(uri, archive_iterator):\n",
    "\n",
    "                    yield res\n",
    "            except ArchiveLoadFailed as exception:\n",
    "                self.warc_input_failed.add(1)\n",
    "                self.get_logger().error(\n",
    "                    'Invalid WARC: {} - {}'.format(uri, exception))\n",
    "            finally:\n",
    "                stream.close()\n",
    "\n",
    "    def process_record(self, record):\n",
    "        raise NotImplementedError('Processing record needs to be customized')\n",
    "\n",
    "    def iterate_records(self, _warc_uri, archive_iterator):\n",
    "        \"\"\"Iterate over all WARC records. This method can be customized\n",
    "           and allows to access also values from ArchiveIterator, namely\n",
    "           WARC record offset and length.\"\"\"\n",
    "    \n",
    "        for record in archive_iterator:\n",
    "            for res in self.process_record(record):\n",
    "              \n",
    "            \n",
    "                yield res\n",
    "         \n",
    "\n",
    "            self.records_processed.add(1)\n",
    "            # WARC record offset and length should be read after the record\n",
    "            # has been processed, otherwise the record content is consumed\n",
    "            # while offset and length are determined:\n",
    "            #  warc_record_offset = archive_iterator.get_record_offset()\n",
    "            #  warc_record_length = archive_iterator.get_record_length()\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wet_text_record(record):\n",
    "        \"\"\"Return true if WARC record is a WET text/plain record\"\"\"\n",
    "        return (record.rec_type == 'conversion' and\n",
    "                record.content_type == 'text/plain')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wat_json_record(record):\n",
    "        \"\"\"Return true if WARC record is a WAT record\"\"\"\n",
    "        return (record.rec_type == 'metadata' and\n",
    "                record.content_type == 'application/json')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_html(record):\n",
    "        \"\"\"Return true if (detected) MIME type of a record is HTML\"\"\"\n",
    "        html_types = ['text/html', 'application/xhtml+xml']\n",
    "        if (('WARC-Identified-Payload-Type' in record.rec_headers) and\n",
    "            (record.rec_headers['WARC-Identified-Payload-Type'] in\n",
    "             html_types)):\n",
    "            return True\n",
    "        for html_type in html_types:\n",
    "            if html_type in record.content_type:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class ServerCountJob(JupyterCCSparkJob):\n",
    "    \"\"\" Count server names sent in HTTP response header\n",
    "        (WARC and WAT is allowed as input)\"\"\"\n",
    "\n",
    "    name = \"CountServers\"\n",
    "    fallback_server_name = '(no server in HTTP header)'\n",
    "\n",
    "    def process_record(self, record):\n",
    "        server_name = None\n",
    "\n",
    "        if self.is_wat_json_record(record):\n",
    "            # WAT (response) record\n",
    "            record = json.loads(record.content_stream().read())\n",
    "            try:\n",
    "                payload = record['Envelope']['Payload-Metadata']\n",
    "                if 'HTTP-Response-Metadata' in payload:\n",
    "                    server_name = payload['HTTP-Response-Metadata'] \\\n",
    "                                         ['Headers'] \\\n",
    "                                         ['Server'] \\\n",
    "                                         .strip()\n",
    "                else:\n",
    "                    # WAT request or metadata records\n",
    "                    return\n",
    "            except KeyError:\n",
    "                pass\n",
    "        elif record.rec_type == 'response':\n",
    "            # WARC response record\n",
    "            server_name = record.http_headers.get_header('server', None)\n",
    "        else:\n",
    "            # warcinfo, request, non-WAT metadata records\n",
    "            return\n",
    "\n",
    "        if server_name and server_name != '':\n",
    "            yield server_name, 1\n",
    "        else:\n",
    "            yield ServerCountJob.fallback_server_name, 1\n",
    "\n",
    "job = ServerCountJob()\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
