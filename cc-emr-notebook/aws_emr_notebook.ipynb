{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Crawl AWS EMR Notebook for processing WARC/WET/WAT/ARC files\n",
    "\n",
    "This notebook will guide you how to:\n",
    "- Configure your EMR Pyspark Kernel\n",
    "- Generate parquets from WARC/WET/WAT/ARC files without spark-submit\n",
    "\n",
    "Requirements:\n",
    "- [AWS EMR Cluster](./cluster_setup.md)\n",
    "- S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start off with configuring our PySpark kernel. We will set our driver memory and executor memory to 1 gigabyte, and we will set the spark profiler to set in the spark configuration. More details with respect to [configuring](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html) and using Spark can be found on the [AWS EMR Release guide](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1G', 'executorMemory': '1G', 'conf': {'spark.python.profile': 'true'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"1G\", \"executorMemory\" : \"1G\", \"conf\": {\"spark.python.profile\": \"true\"} }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simply verify the current sessions configs here to make sure that our driver and executor memory is what it should be, as well as the profiler option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1G', 'executorMemory': '1G', 'conf': {'spark.python.profile': 'true'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606097548163_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-9.ec2.internal:20888/proxy/application_1606097548163_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-188.ec2.internal:8042/node/containerlogs/container_1606097548163_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now store a list of links to the WARC/WET/WAT/ARC files that we want to process. For now we will start with two ARC files. If you are unfamiliar with the format or usage of these links, please be sure to check out the [getting started](https://commoncrawl.org/the-data/get-started/), specifically the section on data location. It should be noted that we are able to load up an arbitrary amount of data locations, and they do not neccessarily have to be ARC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b68d5cf4814f6395cb4a27bbdadb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list = [\"s3://commoncrawl/crawl-001/2008/07/22/3/1216753395382_3.arc.gz\",\n",
    "\"s3://commoncrawl/crawl-002/2009/09/17/12/1253234344648_12.arc.gz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with [cc-pyspark](https://github.com/commoncrawl/cc-pyspark) you may find this familiar. We will define a class \"Args\" here which has some other configurations with respect to how we want the resultant dataframe to look like, and where it will be located. These fields are similar to those that are found in cc-pyspark, except in this case we must do everything programatically (that means no using spark-submit and sending the options by command line!). \n",
    "\n",
    "The important class members that we should note is \"output\". This will be the location to the S3 bucket that you want to write your resultant dataframe to. In this example we will be writing to the `s3://emr-arc-notebook/test_arc_output` bucket.\n",
    "\n",
    "You may also change the input and output partitions. The input partitions are used to partition up the list of data locations (so, partitioning our `file_list`). The output partitions determine how many partitions the resulting dataframe will be in. You can read more about partitions and effective ways to partition data [here](https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65134f078c434bb3847306d889abb5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Args:\n",
    "    warc_parse_http_header = True\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    num_input_partitions = 10\n",
    "    num_output_partitions = 2\n",
    "    output = \"s3://emr-arc-notebook/test_arc_output\"\n",
    "    output_format = \"parquet\"\n",
    "    output_compression = \"gzip\"\n",
    "    output_option = []\n",
    "    local_temp_dir = None\n",
    "    spark_profiler = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the CCSparkJob found in the `sparkcc.py` file of [cc-pyspark](https://github.com/commoncrawl/cc-pyspark/blob/master/sparkcc.py). First we retrieve the SparkContext that we have (from the PySpark kernel provided by AWS EMR, which we configured earlier). Then, we run our job by partitioning the input files. For each input file, we run the `ArchiveIterator` object on it, provided by the `warcio` library which allows us to process WARC/WAT/WET/ARC files. We run this on our `ServerCountJob` class, although we could use any class that inherits from the `CCSparkJob` class found [here](https://github.com/commoncrawl/cc-pyspark) (for example, `word_count.py`, `wat_extract_links.py`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2156299fbe84a1eb31c3fe928fa8431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "from io import BytesIO\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.recordloader import ArchiveLoadFailed\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "import tempfile\n",
    "\n",
    "LOGGING_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n",
    "\n",
    "    \n",
    "class JupyterCCSparkJob(object):\n",
    "    \"\"\"\n",
    "    A simple Spark job definition to process Common Crawl data\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'CCSparkJob'\n",
    "\n",
    "    output_schema = StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"val\", LongType(), True)\n",
    "    ])\n",
    "\n",
    "    # description of input and output shown in --help\n",
    "    input_descr = \"Path to file listing input paths\"\n",
    "    output_descr = \"Name of output table (saved in spark.sql.warehouse.dir)\"\n",
    "\n",
    "    warc_parse_http_header = True\n",
    "\n",
    "    args = None\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    log_level = 'INFO'\n",
    "    logging.basicConfig(level=log_level, format=LOGGING_FORMAT)\n",
    "\n",
    "\n",
    "    def parse_arguments(self):\n",
    "        \"\"\" Returns the parsed arguments from the command line \"\"\"\n",
    "\n",
    "        description = self.name\n",
    "        if self.__doc__ is not None:\n",
    "            description += \" - \"\n",
    "            description += self.__doc__\n",
    "        args = Args()\n",
    "        return args\n",
    "\n",
    "    def add_arguments(self, parser):\n",
    "        pass\n",
    "\n",
    "    def validate_arguments(self, args):\n",
    "        if \"orc\" == args.output_format and \"gzip\" == args.output_compression:\n",
    "            # gzip for Parquet, zlib for ORC\n",
    "            args.output_compression = \"zlib\"\n",
    "        return True\n",
    "\n",
    "    def get_output_options(self):\n",
    "        return {x[0]: x[1] for x in map(lambda x: x.split('=', 1),\n",
    "                                        self.args.output_option)}\n",
    "\n",
    "    def init_logging(self, level=None):\n",
    "        if level is None:\n",
    "            level = self.log_level\n",
    "        else:\n",
    "            self.log_level = level\n",
    "        logging.basicConfig(level=level, format=LOGGING_FORMAT)\n",
    "\n",
    "    def init_accumulators(self, sc):\n",
    "        self.records_processed = sc.accumulator(0)\n",
    "        self.warc_input_processed = sc.accumulator(0)\n",
    "        self.warc_input_failed = sc.accumulator(0)\n",
    "\n",
    "    def get_logger(self, spark_context=None):\n",
    "        \"\"\"Get logger from SparkContext or (if None) from logging module\"\"\"\n",
    "        if spark_context is None:\n",
    "            return logging.getLogger(self.name)\n",
    "        return spark_context._jvm.org.apache.log4j.LogManager \\\n",
    "            .getLogger(self.name)\n",
    "\n",
    "    def run(self):\n",
    "        self.args = self.parse_arguments()\n",
    "        \n",
    "        conf = SparkConf()\n",
    "        \n",
    "        if self.args.spark_profiler:\n",
    "            conf = conf.set(\"spark.python.profile\", \"true\")\n",
    "        \n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        \n",
    "        \n",
    "        sqlc = SQLContext(sparkContext=sc)\n",
    "\n",
    "        self.init_accumulators(sc)\n",
    "\n",
    "        self.run_job(sc, sqlc)\n",
    "\n",
    "        if self.args.spark_profiler:\n",
    "            sc.show_profiles()\n",
    "\n",
    "        sc.stop()\n",
    "\n",
    "    def log_aggregator(self, sc, agg, descr):\n",
    "        self.get_logger(sc).info(descr.format(agg.value))\n",
    "\n",
    "    def log_aggregators(self, sc):\n",
    "        self.log_aggregator(sc, self.warc_input_processed,\n",
    "                            'WARC/WAT/WET input files processed = {}')\n",
    "        self.log_aggregator(sc, self.warc_input_failed,\n",
    "                            'WARC/WAT/WET input files failed = {}')\n",
    "        self.log_aggregator(sc, self.records_processed,\n",
    "                            'WARC/WAT/WET records processed = {}')\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_by_key_func(a, b):\n",
    "        return a + b\n",
    "\n",
    "    def run_job(self, sc, sqlc):\n",
    "        \n",
    "        input_data = sc.parallelize(file_list)\n",
    "           \n",
    "        output = input_data.mapPartitionsWithIndex(self.process_warcs)\n",
    "        sqlc.createDataFrame(output, schema=self.output_schema) \\\n",
    "            .coalesce(self.args.num_output_partitions) \\\n",
    "            .write \\\n",
    "            .format(self.args.output_format) \\\n",
    "            .option(\"compression\", self.args.output_compression) \\\n",
    "            .options(**self.get_output_options()) \\\n",
    "            .parquet(self.args.output)\n",
    "    \n",
    "        self.log_aggregators(sc)\n",
    "    \n",
    "    def process_warcs(self, id_, iterator):\n",
    "        s3pattern = re.compile('^s3://([^/]+)/(.+)')\n",
    "        base_dir = \"/user/\"\n",
    "\n",
    "        # S3 client (not thread-safe, initialize outside parallelized loop)\n",
    "        s3client = boto3.client('s3')\n",
    "       \n",
    "        for uri in iterator:\n",
    "            self.warc_input_processed.add(1)\n",
    "            if uri.startswith('s3://'):\n",
    "                self.get_logger().info('Reading from S3 {}'.format(uri))\n",
    "                s3match = s3pattern.match(uri)\n",
    "                if s3match is None:\n",
    "                    self.get_logger().error(\"Invalid S3 URI: \" + uri)\n",
    "                    continue\n",
    "                bucketname = s3match.group(1)\n",
    "                path = s3match.group(2)\n",
    "                warctemp = TemporaryFile(mode='w+b',\n",
    "                                         dir=self.args.local_temp_dir)\n",
    "                try:\n",
    "                    s3client.download_fileobj(bucketname, path, warctemp)\n",
    "                except botocore.client.ClientError as exception:\n",
    "                    self.get_logger().error(\n",
    "                        'Failed to download {}: {}'.format(uri, exception))\n",
    "                    self.warc_input_failed.add(1)\n",
    "                    warctemp.close()\n",
    "                    continue\n",
    "                warctemp.seek(0)\n",
    "                stream = warctemp\n",
    "            elif uri.startswith('hdfs://'):\n",
    "                self.get_logger().error(\"HDFS input not implemented: \" + uri)\n",
    "                continue\n",
    "            else:\n",
    "                self.get_logger().info('Reading local stream {}'.format(uri))\n",
    "                if uri.startswith('file:'):\n",
    "                    uri = uri[5:]\n",
    "                uri = os.path.join(base_dir, uri)\n",
    "                try:\n",
    "                    stream = open(uri, 'rb')\n",
    "                except IOError as exception:\n",
    "                    self.get_logger().error(\n",
    "                        'Failed to open {}: {}'.format(uri, exception))\n",
    "                    self.warc_input_failed.add(1)\n",
    "                    continue\n",
    "\n",
    "            no_parse = (not self.warc_parse_http_header)\n",
    "           \n",
    "            try:\n",
    "                archive_iterator = ArchiveIterator(stream,\n",
    "                                                   no_record_parse=no_parse, arc2warc = True)\n",
    "                \n",
    "                for res in self.iterate_records(uri, archive_iterator):\n",
    "\n",
    "                    yield res\n",
    "            except ArchiveLoadFailed as exception:\n",
    "                self.warc_input_failed.add(1)\n",
    "                self.get_logger().error(\n",
    "                    'Invalid WARC: {} - {}'.format(uri, exception))\n",
    "            finally:\n",
    "                stream.close()\n",
    "\n",
    "    def process_record(self, record):\n",
    "        raise NotImplementedError('Processing record needs to be customized')\n",
    "\n",
    "    def iterate_records(self, _warc_uri, archive_iterator):\n",
    "        \"\"\"Iterate over all WARC records. This method can be customized\n",
    "           and allows to access also values from ArchiveIterator, namely\n",
    "           WARC record offset and length.\"\"\"\n",
    "    \n",
    "        for record in archive_iterator:\n",
    "            for res in self.process_record(record):\n",
    "              \n",
    "            \n",
    "                yield res\n",
    "         \n",
    "\n",
    "            self.records_processed.add(1)\n",
    "            # WARC record offset and length should be read after the record\n",
    "            # has been processed, otherwise the record content is consumed\n",
    "            # while offset and length are determined:\n",
    "            #  warc_record_offset = archive_iterator.get_record_offset()\n",
    "            #  warc_record_length = archive_iterator.get_record_length()\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wet_text_record(record):\n",
    "        \"\"\"Return true if WARC record is a WET text/plain record\"\"\"\n",
    "        return (record.rec_type == 'conversion' and\n",
    "                record.content_type == 'text/plain')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wat_json_record(record):\n",
    "        \"\"\"Return true if WARC record is a WAT record\"\"\"\n",
    "        return (record.rec_type == 'metadata' and\n",
    "                record.content_type == 'application/json')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_html(record):\n",
    "        \"\"\"Return true if (detected) MIME type of a record is HTML\"\"\"\n",
    "        html_types = ['text/html', 'application/xhtml+xml']\n",
    "        if (('WARC-Identified-Payload-Type' in record.rec_headers) and\n",
    "            (record.rec_headers['WARC-Identified-Payload-Type'] in\n",
    "             html_types)):\n",
    "            return True\n",
    "        for html_type in html_types:\n",
    "            if html_type in record.content_type:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class ServerCountJob(JupyterCCSparkJob):\n",
    "    \"\"\" Count server names sent in HTTP response header\n",
    "        (WARC and WAT is allowed as input)\"\"\"\n",
    "\n",
    "    name = \"CountServers\"\n",
    "    fallback_server_name = '(no server in HTTP header)'\n",
    "\n",
    "    def process_record(self, record):\n",
    "        server_name = None\n",
    "\n",
    "        if self.is_wat_json_record(record):\n",
    "            # WAT (response) record\n",
    "            record = json.loads(record.content_stream().read())\n",
    "            try:\n",
    "                payload = record['Envelope']['Payload-Metadata']\n",
    "                if 'HTTP-Response-Metadata' in payload:\n",
    "                    server_name = payload['HTTP-Response-Metadata'] \\\n",
    "                                         ['Headers'] \\\n",
    "                                         ['Server'] \\\n",
    "                                         .strip()\n",
    "                else:\n",
    "                    # WAT request or metadata records\n",
    "                    return\n",
    "            except KeyError:\n",
    "                pass\n",
    "        elif record.rec_type == 'response':\n",
    "            # WARC response record\n",
    "            server_name = record.http_headers.get_header('server', None)\n",
    "        else:\n",
    "            # warcinfo, request, non-WAT metadata records\n",
    "            return\n",
    "\n",
    "        if server_name and server_name != '':\n",
    "            yield server_name, 1\n",
    "        else:\n",
    "            yield ServerCountJob.fallback_server_name, 1\n",
    "\n",
    "job = ServerCountJob()\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created some parquet files, let's learn how to process them in the [dataframe analysis notebook](dataframe_analysis.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
